{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1032238,"sourceType":"datasetVersion","datasetId":568973},{"sourceId":7212536,"sourceType":"datasetVersion","datasetId":4173507}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout, LSTM\nfrom sklearn.model_selection import train_test_split\nimport librosa\nimport numpy as np\nimport os\n\n# Function to load and preprocess images\ndef load_and_preprocess_image(image_path):\n    img = image.load_img(image_path, target_size=(224, 224))\n    img_array = image.img_to_array(img)\n    return img_array/255.0\n\ndef load_and_preprocess_audio(audio_path, max_audio_length):\n    audio_data, _ = librosa.load(audio_path, sr=SAMPLE_RATE)\n\n    # Ensure the audio has the desired length\n    if len(audio_data) < max_audio_length:\n        # If too short, pad with zeros\n        audio_data = np.pad(audio_data, (0, max_audio_length - len(audio_data)))\n    else:\n        audio_data = audio_data[:max_audio_length]\n    mfccs = librosa.feature.mfcc(y=audio_data, sr=SAMPLE_RATE, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n\n    return mfccs","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:19:42.655793Z","iopub.execute_input":"2023-12-18T06:19:42.656721Z","iopub.status.idle":"2023-12-18T06:19:54.941019Z","shell.execute_reply.started":"2023-12-18T06:19:42.656676Z","shell.execute_reply":"2023-12-18T06:19:54.940200Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import librosa.util\n\n# Constants\nSAMPLE_RATE = 5000\nTRACK_DURATION = 30\nSAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n\nnum_mfcc = 13\nn_fft = 1024\nhop_length = 256\nnum_segments = 15\n\n# Calculate the maximum audio length in samples\nmax_audio_length = SAMPLES_PER_TRACK * num_segments","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:19:54.942599Z","iopub.execute_input":"2023-12-18T06:19:54.943126Z","iopub.status.idle":"2023-12-18T06:19:54.950142Z","shell.execute_reply.started":"2023-12-18T06:19:54.943098Z","shell.execute_reply":"2023-12-18T06:19:54.949283Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import random\n# Function to create combinations of image and audio data\ndef create_data_combinations(image_folder, audio_folder, max_audio_length):\n    images,labels,voices=[],[],[]\n    class_mapping = {\n    'disco': 0,\n    'metal': 1,\n    'reggae': 2,\n    'blues': 3,\n    'rock': 4,\n    'classical': 5,\n    'jazz': 6,\n    'hiphop': 7,\n    'country': 8,\n    'pop': 9\n    }\n\n    for class_folder in os.listdir(image_folder):\n        class_path = os.path.join(image_folder, class_folder)\n        i=0\n        for image_name in os.listdir(class_path):\n            image_path = os.path.join(class_path, image_name)\n            audio_path = os.path.join(audio_folder,class_folder, class_folder)\n            # Load and preprocess image and audio data\n            img_data = load_and_preprocess_image(image_path)\n            audio_path = os.path.join(audio_folder, class_folder)\n            audio_files = os.listdir(audio_path)\n            selected_audio_files = random.sample(audio_files, min(5, len(audio_files)))\n            for audio in selected_audio_files:\n                try:\n                    data_path=os.path.join(audio_path,audio)\n                    audio_data = load_and_preprocess_audio(data_path, max_audio_length)\n                    images.append(img_data)\n                    voices.append(audio_data)\n                    label = class_mapping[class_folder]\n                    labels.append(label)\n                except:\n                    continue\n            if i==60:\n                break\n            i=i+1\n        print(class_folder)\n\n    return images,voices,labels\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:19:54.951167Z","iopub.execute_input":"2023-12-18T06:19:54.951431Z","iopub.status.idle":"2023-12-18T06:19:54.974683Z","shell.execute_reply.started":"2023-12-18T06:19:54.951406Z","shell.execute_reply":"2023-12-18T06:19:54.973710Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/input/music-recomend-data/music_recomendation_Data/music_recomendation_Data\")\n    # Example: Create data combinations from image and audio folders\nimage_folder = 'images_original'\naudio_folder = 'genres_original'\nimages,voices,labels = create_data_combinations(image_folder, audio_folder,max_audio_length)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:19:54.976346Z","iopub.execute_input":"2023-12-18T06:19:54.976675Z","iopub.status.idle":"2023-12-18T06:30:30.549315Z","shell.execute_reply.started":"2023-12-18T06:19:54.976649Z","shell.execute_reply":"2023-12-18T06:30:30.545866Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"disco\nmetal\nreggae\nblues\nrock\nclassical\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_42/3730052120.py:19: UserWarning: PySoundFile failed. Trying audioread instead.\n  audio_data, _ = librosa.load(audio_path, sr=SAMPLE_RATE)\n/opt/conda/lib/python3.10/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n\tDeprecated as of librosa version 0.10.0.\n\tIt will be removed in librosa version 1.0.\n  y, sr_native = __audioread_load(path, offset, duration, dtype)\n/tmp/ipykernel_42/3730052120.py:19: UserWarning: PySoundFile failed. Trying audioread instead.\n  audio_data, _ = librosa.load(audio_path, sr=SAMPLE_RATE)\n/opt/conda/lib/python3.10/site-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n\tDeprecated as of librosa version 0.10.0.\n\tIt will be removed in librosa version 1.0.\n  y, sr_native = __audioread_load(path, offset, duration, dtype)\n","output_type":"stream"},{"name":"stdout","text":"jazz\nhiphop\ncountry\npop\n","output_type":"stream"}]},{"cell_type":"code","source":"# Split data into features and labels\nX_image = np.array(images)\nX_audio = np.array(voices)\nY_labels = np.array(labels)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:32:43.309793Z","iopub.execute_input":"2023-12-18T06:32:43.310547Z","iopub.status.idle":"2023-12-18T06:32:44.145015Z","shell.execute_reply.started":"2023-12-18T06:32:43.310513Z","shell.execute_reply":"2023-12-18T06:32:44.144193Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_audio.shape,X_image.shape,Y_labels.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:32:44.146700Z","iopub.execute_input":"2023-12-18T06:32:44.146982Z","iopub.status.idle":"2023-12-18T06:32:44.154163Z","shell.execute_reply.started":"2023-12-18T06:32:44.146957Z","shell.execute_reply":"2023-12-18T06:32:44.153320Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"((3048, 13, 8790), (3048, 224, 224, 3), (3048,))"},"metadata":{}}]},{"cell_type":"code","source":"print(np.unique(Y_labels))","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:32:44.155200Z","iopub.execute_input":"2023-12-18T06:32:44.155497Z","iopub.status.idle":"2023-12-18T06:32:44.164981Z","shell.execute_reply.started":"2023-12-18T06:32:44.155439Z","shell.execute_reply":"2023-12-18T06:32:44.164057Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[0 1 2 3 4 5 6 7 8 9]\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\nY_labels = to_categorical(Y_labels, num_classes=10)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:32:44.391039Z","iopub.execute_input":"2023-12-18T06:32:44.391392Z","iopub.status.idle":"2023-12-18T06:32:44.398496Z","shell.execute_reply.started":"2023-12-18T06:32:44.391364Z","shell.execute_reply":"2023-12-18T06:32:44.397638Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_image_train, X_image_val, X_audio_train, X_audio_val, y_train, y_val = train_test_split(\n    X_image, X_audio, Y_labels, test_size=0.2, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:32:57.314759Z","iopub.execute_input":"2023-12-18T06:32:57.315136Z","iopub.status.idle":"2023-12-18T06:32:58.214684Z","shell.execute_reply.started":"2023-12-18T06:32:57.315106Z","shell.execute_reply":"2023-12-18T06:32:58.213868Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_image_train.shape,X_audio_train.shape,y_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:32:58.216153Z","iopub.execute_input":"2023-12-18T06:32:58.216440Z","iopub.status.idle":"2023-12-18T06:32:58.222524Z","shell.execute_reply.started":"2023-12-18T06:32:58.216415Z","shell.execute_reply":"2023-12-18T06:32:58.221686Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"((2438, 224, 224, 3), (2438, 13, 8790), (2438, 10))"},"metadata":{}}]},{"cell_type":"code","source":"X_image_val.shape,X_audio_val.shape,y_val.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:32:58.223794Z","iopub.execute_input":"2023-12-18T06:32:58.224120Z","iopub.status.idle":"2023-12-18T06:32:58.234377Z","shell.execute_reply.started":"2023-12-18T06:32:58.224088Z","shell.execute_reply":"2023-12-18T06:32:58.233573Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"((610, 224, 224, 3), (610, 13, 8790), (610, 10))"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, LSTM, concatenate\n\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input, decode_predictions\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.densenet import DenseNet121\n\n# Load pre-trained VGG16 model with weights trained on ImageNet\nvgg_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the layers of VGG16\nfor layer in vgg_model.layers:\n    layer.trainable = False\n\n# Define the shape of the input data\naudio_input_shape = (num_mfcc, X_audio.shape[2])  # (number of MFCC coefficients, audio length)\nimage_input_shape = X_image.shape[1:]  # (image height, image width, number of channels)\n\n# Define the input layers\naudio_input = Input(shape=audio_input_shape, name='audio_input')\nimage_input = Input(shape=image_input_shape, name='image_input')\n\n# Audio processing\naudio_lstm = LSTM(64)(audio_input)\naudio_output = Dense(32, activation='relu')(audio_lstm)\n\n# Image processing using VGG16\nimage_vgg = vgg_model(image_input)\nimage_flatten = Flatten()(image_vgg)\nimage_output = Dense(512, activation='relu')(image_flatten)\n\n# Concatenate the outputs of audio and image processing\nmerged = concatenate([audio_output, image_output])\nmerged = Dense(512, activation='relu')(merged)\n# Final output layer\noutput = Dense(10, activation='softmax')(merged)  # Assuming a binary classification task\n\n# Create the model\nmodel = Model(inputs=[image_input,audio_input, ], outputs=output)\n\n# Compile the model (specify the appropriate loss and optimizer based on your task)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Display the model summary\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:32:58.533388Z","iopub.execute_input":"2023-12-18T06:32:58.534220Z","iopub.status.idle":"2023-12-18T06:33:05.711196Z","shell.execute_reply.started":"2023-12-18T06:32:58.534190Z","shell.execute_reply":"2023-12-18T06:33:05.710309Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n87910968/87910968 [==============================] - 0s 0us/step\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n image_input (InputLayer)    [(None, 224, 224, 3)]        0         []                            \n                                                                                                  \n audio_input (InputLayer)    [(None, 13, 8790)]           0         []                            \n                                                                                                  \n inception_v3 (Functional)   (None, 5, 5, 2048)           2180278   ['image_input[0][0]']         \n                                                          4                                       \n                                                                                                  \n lstm (LSTM)                 (None, 64)                   2266880   ['audio_input[0][0]']         \n                                                                                                  \n flatten (Flatten)           (None, 51200)                0         ['inception_v3[0][0]']        \n                                                                                                  \n dense (Dense)               (None, 32)                   2080      ['lstm[0][0]']                \n                                                                                                  \n dense_1 (Dense)             (None, 512)                  2621491   ['flatten[0][0]']             \n                                                          2                                       \n                                                                                                  \n concatenate_2 (Concatenate  (None, 544)                  0         ['dense[0][0]',               \n )                                                                   'dense_1[0][0]']             \n                                                                                                  \n dense_2 (Dense)             (None, 512)                  279040    ['concatenate_2[0][0]']       \n                                                                                                  \n dense_3 (Dense)             (None, 10)                   5130      ['dense_2[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 50570826 (192.91 MB)\nTrainable params: 28768042 (109.74 MB)\nNon-trainable params: 21802784 (83.17 MB)\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\nmodel.fit(\n    [X_image_train, X_audio_train],\n    y_train,\n    epochs=40,\n    validation_data=([X_image_val, X_audio_val], y_val),\n    batch_size=2\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:33:56.847691Z","iopub.execute_input":"2023-12-18T06:33:56.848063Z","iopub.status.idle":"2023-12-18T06:52:11.209892Z","shell.execute_reply.started":"2023-12-18T06:33:56.848033Z","shell.execute_reply":"2023-12-18T06:52:11.208942Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1/40\n1219/1219 [==============================] - 44s 25ms/step - loss: 2.9000 - accuracy: 0.3171 - val_loss: 1.8246 - val_accuracy: 0.2984\nEpoch 2/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 1.8890 - accuracy: 0.3142 - val_loss: 1.8494 - val_accuracy: 0.3443\nEpoch 3/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 1.7445 - accuracy: 0.3683 - val_loss: 1.7240 - val_accuracy: 0.4016\nEpoch 4/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 1.6007 - accuracy: 0.4352 - val_loss: 1.6652 - val_accuracy: 0.4016\nEpoch 5/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 1.4251 - accuracy: 0.4967 - val_loss: 1.5105 - val_accuracy: 0.4607\nEpoch 6/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 1.3389 - accuracy: 0.5295 - val_loss: 1.6490 - val_accuracy: 0.4672\nEpoch 7/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 1.2921 - accuracy: 0.5574 - val_loss: 1.5264 - val_accuracy: 0.5049\nEpoch 8/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 1.1761 - accuracy: 0.5935 - val_loss: 1.3692 - val_accuracy: 0.5492\nEpoch 9/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 1.1118 - accuracy: 0.6202 - val_loss: 1.2862 - val_accuracy: 0.5869\nEpoch 10/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 1.0299 - accuracy: 0.6587 - val_loss: 1.2304 - val_accuracy: 0.6295\nEpoch 11/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.8829 - accuracy: 0.6989 - val_loss: 1.1193 - val_accuracy: 0.6508\nEpoch 12/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.8743 - accuracy: 0.7129 - val_loss: 1.0694 - val_accuracy: 0.6557\nEpoch 13/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.9782 - accuracy: 0.6669 - val_loss: 1.0790 - val_accuracy: 0.6475\nEpoch 14/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.8102 - accuracy: 0.7264 - val_loss: 1.0065 - val_accuracy: 0.6967\nEpoch 15/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.7423 - accuracy: 0.7502 - val_loss: 1.0806 - val_accuracy: 0.6951\nEpoch 16/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.8163 - accuracy: 0.7244 - val_loss: 1.1157 - val_accuracy: 0.6738\nEpoch 17/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.7490 - accuracy: 0.7445 - val_loss: 1.0158 - val_accuracy: 0.6918\nEpoch 18/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.6905 - accuracy: 0.7785 - val_loss: 0.9814 - val_accuracy: 0.7361\nEpoch 19/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.6118 - accuracy: 0.8019 - val_loss: 0.9476 - val_accuracy: 0.7459\nEpoch 20/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.5632 - accuracy: 0.8187 - val_loss: 0.9112 - val_accuracy: 0.7541\nEpoch 21/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.6567 - accuracy: 0.7920 - val_loss: 0.9119 - val_accuracy: 0.7459\nEpoch 22/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.5872 - accuracy: 0.8052 - val_loss: 0.8707 - val_accuracy: 0.7770\nEpoch 23/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.4647 - accuracy: 0.8437 - val_loss: 0.8664 - val_accuracy: 0.7984\nEpoch 24/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.4633 - accuracy: 0.8527 - val_loss: 0.8365 - val_accuracy: 0.7787\nEpoch 25/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.4777 - accuracy: 0.8503 - val_loss: 0.9167 - val_accuracy: 0.7918\nEpoch 26/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.5148 - accuracy: 0.8285 - val_loss: 0.8707 - val_accuracy: 0.7787\nEpoch 27/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.5235 - accuracy: 0.8310 - val_loss: 0.8678 - val_accuracy: 0.7689\nEpoch 28/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.5252 - accuracy: 0.8339 - val_loss: 0.8237 - val_accuracy: 0.7721\nEpoch 29/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.5301 - accuracy: 0.8240 - val_loss: 0.7570 - val_accuracy: 0.7738\nEpoch 30/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.3724 - accuracy: 0.8839 - val_loss: 0.6716 - val_accuracy: 0.8262\nEpoch 31/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.3068 - accuracy: 0.9024 - val_loss: 0.8273 - val_accuracy: 0.8295\nEpoch 32/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.3451 - accuracy: 0.8975 - val_loss: 0.7947 - val_accuracy: 0.8279\nEpoch 33/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.3439 - accuracy: 0.8897 - val_loss: 0.7872 - val_accuracy: 0.8131\nEpoch 34/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.3791 - accuracy: 0.8847 - val_loss: 0.8371 - val_accuracy: 0.8148\nEpoch 35/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.4016 - accuracy: 0.8774 - val_loss: 1.0221 - val_accuracy: 0.7754\nEpoch 36/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.4420 - accuracy: 0.8634 - val_loss: 0.8862 - val_accuracy: 0.7951\nEpoch 37/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.3145 - accuracy: 0.9036 - val_loss: 0.9179 - val_accuracy: 0.8213\nEpoch 38/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.3511 - accuracy: 0.8966 - val_loss: 0.8611 - val_accuracy: 0.8361\nEpoch 39/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.3328 - accuracy: 0.8991 - val_loss: 0.7493 - val_accuracy: 0.8508\nEpoch 40/40\n1219/1219 [==============================] - 27s 22ms/step - loss: 0.2468 - accuracy: 0.9192 - val_loss: 0.8226 - val_accuracy: 0.8295\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x79db5c0ed3f0>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model\ntest_loss, test_acc = model.evaluate([X_image_val, X_audio_val], y_val)\nprint(f\"Test accuracy: {test_acc}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:59:04.282124Z","iopub.execute_input":"2023-12-18T06:59:04.282841Z","iopub.status.idle":"2023-12-18T06:59:10.059689Z","shell.execute_reply.started":"2023-12-18T06:59:04.282808Z","shell.execute_reply":"2023-12-18T06:59:10.058823Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"20/20 [==============================] - 4s 51ms/step - loss: 0.8226 - accuracy: 0.8295\nTest accuracy: 0.8295081853866577\n","output_type":"stream"}]},{"cell_type":"code","source":"    class_mapping = {\n    'disco': 0,\n    'metal': 1,\n    'reggae': 2,\n    'blues': 3,\n    'rock': 4,\n    'classical': 5,\n    'jazz': 6,\n    'hiphop': 7,\n    'country': 8,\n    'pop': 9\n    }","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:59:15.888472Z","iopub.execute_input":"2023-12-18T06:59:15.889180Z","iopub.status.idle":"2023-12-18T06:59:15.894099Z","shell.execute_reply.started":"2023-12-18T06:59:15.889139Z","shell.execute_reply":"2023-12-18T06:59:15.893168Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n# Save the model\nmodel.save('/kaggle/working/InceptionV3_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-12-18T06:59:38.277541Z","iopub.execute_input":"2023-12-18T06:59:38.277916Z","iopub.status.idle":"2023-12-18T06:59:39.790378Z","shell.execute_reply.started":"2023-12-18T06:59:38.277886Z","shell.execute_reply":"2023-12-18T06:59:39.788524Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the model\nloaded_model = load_model('/kaggle/working/InceptionV3_model.h5')\n\n# Specify the paths to the new audio and image files\nnew_audio_path = '/kaggle/input/music-recomend-data/music_recomendation_Data/music_recomendation_Data/genres_original/disco/disco.00026.wav'\nnew_image_path = '/kaggle/input/music-recomend-data/music_recomendation_Data/music_recomendation_Data/images_original/disco/disco00026.png'\n\n\n# Load and preprocess the single image and audio data\nnew_img_data = load_and_preprocess_image(new_image_path)\nnew_audio_data = load_and_preprocess_audio(new_audio_path, max_audio_length)\n\n# Reshape the data to match the model input shape\nnew_img_data = np.expand_dims(new_img_data, axis=0)\nnew_audio_data = np.expand_dims(new_audio_data, axis=0)\n\n# Make a prediction using the loaded model\nprediction = loaded_model.predict([new_img_data, new_audio_data])\n\n# Get the predicted label\npredicted_label = np.argmax(prediction)\n\nprint(f'The predicted label is: {predicted_label}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T07:00:53.431564Z","iopub.execute_input":"2023-12-18T07:00:53.431958Z","iopub.status.idle":"2023-12-18T07:01:00.522983Z","shell.execute_reply.started":"2023-12-18T07:00:53.431928Z","shell.execute_reply":"2023-12-18T07:01:00.521982Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 2s 2s/step\nThe predicted label is: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\n# Assuming you have trained your model and obtained predictions on the validation set\npredictions = model.predict([X_image_val, X_audio_val])\n\npredicted_labels = np.argmax(predictions, axis=1)\nactual_labels = np.argmax(y_val, axis=1)\n# Create a list of class labels\nclass_labels = list(class_mapping.keys())\nprint()\n# Generate confusion matrix with class names\nconf_matrix = confusion_matrix(actual_labels, predicted_labels)\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\n# Generate classification report with class names\nclass_report = classification_report(actual_labels, predicted_labels, target_names=class_labels)\nprint(\"Classification Report:\")\nprint(class_report)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-18T07:01:36.106215Z","iopub.execute_input":"2023-12-18T07:01:36.106762Z","iopub.status.idle":"2023-12-18T07:01:40.267878Z","shell.execute_reply.started":"2023-12-18T07:01:36.106724Z","shell.execute_reply":"2023-12-18T07:01:40.266957Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"20/20 [==============================] - 3s 44ms/step\n\nConfusion Matrix:\n[[49  3  1  2  1  0  4  1  2  4]\n [ 4 58  1  2  0  0  0  1  4  4]\n [ 2  0 50  0  0  0  3  2  1  2]\n [ 0  5  3 45  2  0  2  1  1  1]\n [ 1  2  2  2 47  0  0  2  1  1]\n [ 0  0  1  0  0 56  0  0  0  0]\n [ 1  2  0  2  0  0 49  0  0  2]\n [ 0  1  4  1  1  0  1 45  1  0]\n [ 0  1  0  2  3  0  1  0 57  2]\n [ 1  0  1  0  1  0  2  1  2 50]]\nClassification Report:\n              precision    recall  f1-score   support\n\n       disco       0.84      0.73      0.78        67\n       metal       0.81      0.78      0.79        74\n      reggae       0.79      0.83      0.81        60\n       blues       0.80      0.75      0.78        60\n        rock       0.85      0.81      0.83        58\n   classical       1.00      0.98      0.99        57\n        jazz       0.79      0.88      0.83        56\n      hiphop       0.85      0.83      0.84        54\n     country       0.83      0.86      0.84        66\n         pop       0.76      0.86      0.81        58\n\n    accuracy                           0.83       610\n   macro avg       0.83      0.83      0.83       610\nweighted avg       0.83      0.83      0.83       610\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}